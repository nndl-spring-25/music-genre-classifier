{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9056db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Use Agg backend for headless plotting\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a504e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.11).\n"
     ]
    }
   ],
   "source": [
    "# Set up parameters\n",
    "GENRES = [\n",
    "    \"blues\", \"classical\", \"country\", \"disco\", \"hiphop\",\n",
    "    \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"\n",
    "]\n",
    "SR = 22050\n",
    "SEGMENT_SECONDS = 5\n",
    "SEGMENT_SAMPLES = SEGMENT_SECONDS * SR\n",
    "DATA_ROOT = kagglehub.dataset_download(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n",
    "SPEC_ROOT = \"./spectrograms\"\n",
    "IMG_SIZE = (224, 224)  # input size for ResNet\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LEARNING_RATES = [1e-3, 1e-4]  # Try multiple learning rates\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "K_FOLDS = 5\n",
    "BEST_MODEL_PATH = \"./best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional audio augmentation\n",
    "def augment_audio(y, sr):\n",
    "    y = librosa.effects.time_stretch(y, rate=random.uniform(0.9, 1.1))\n",
    "    y = librosa.effects.pitch_shift(y, sr=sr, n_steps=random.randint(-2, 2))\n",
    "    y += np.random.normal(0, 0.005, y.shape)\n",
    "    return y\n",
    "\n",
    "# Create full-length spectrograms with both original and augmented audio\n",
    "def generate_spectrograms():\n",
    "    for genre in GENRES:\n",
    "        os.makedirs(f\"{SPEC_ROOT}/{genre}\", exist_ok=True)\n",
    "        for i in tqdm(range(100), desc=f\"{genre:10s}\"):\n",
    "            file_path = os.path.join(DATA_ROOT, \"Data\", \"genres_original\", genre, f\"{genre}.000{i:02d}.wav\")\n",
    "            try:\n",
    "                y, _ = librosa.load(file_path, sr=SR)\n",
    "                if y is None or len(y) == 0:\n",
    "                    raise ValueError(\"Empty audio signal\")\n",
    "\n",
    "                for suffix, audio in zip([\"orig\", \"aug\"], [y, augment_audio(y.copy(), SR)]):\n",
    "                    mel_spec = librosa.feature.melspectrogram(y=audio, sr=SR, n_mels=128)\n",
    "                    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "                    if mel_db.shape[1] == 0:\n",
    "                        raise ValueError(\"Empty spectrogram\")\n",
    "\n",
    "                    fig = plt.figure(figsize=(2.24, 2.24), dpi=100)\n",
    "                    librosa.display.specshow(mel_db, sr=SR, x_axis=None, y_axis=None)\n",
    "                    plt.axis('off')\n",
    "                    plt.tight_layout()\n",
    "                    filename = f\"{genre}_{i:02d}_{suffix}.png\"\n",
    "                    out_path = f\"{SPEC_ROOT}/{genre}/{filename}\"\n",
    "\n",
    "                    try:\n",
    "                        plt.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
    "                        if os.path.getsize(out_path) == 0:\n",
    "                            raise IOError(\"Output PNG file is 0 bytes\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving {out_path}: {e}\")\n",
    "                    finally:\n",
    "                        plt.close(fig)\n",
    "                        plt.cla()\n",
    "                        plt.clf()\n",
    "                        gc.collect()\n",
    "                        time.sleep(0.05)  # allow WSL to breathe\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a02cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-based model for classification\n",
    "class SpectrogramResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee68f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347f4e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating spectrograms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blues     :   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blues     : 100%|██████████| 100/100 [02:28<00:00,  1.49s/it]\n",
      "classical : 100%|██████████| 100/100 [02:27<00:00,  1.47s/it]\n",
      "country   : 100%|██████████| 100/100 [02:32<00:00,  1.53s/it]\n",
      "disco     : 100%|██████████| 100/100 [02:37<00:00,  1.58s/it]\n",
      "hiphop    : 100%|██████████| 100/100 [02:38<00:00,  1.58s/it]\n",
      "jazz      : 100%|██████████| 100/100 [02:34<00:00,  1.55s/it]\n",
      "metal     : 100%|██████████| 100/100 [02:33<00:00,  1.53s/it]\n",
      "pop       : 100%|██████████| 100/100 [02:29<00:00,  1.50s/it]\n",
      "reggae    : 100%|██████████| 100/100 [02:36<00:00,  1.57s/it]\n",
      "rock      : 100%|██████████| 100/100 [02:36<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# Check for completeness\n",
    "def should_generate_spectrograms():\n",
    "    if not os.path.exists(SPEC_ROOT):\n",
    "        return True\n",
    "    for genre in GENRES:\n",
    "        genre_dir = os.path.join(SPEC_ROOT, genre)\n",
    "        if not os.path.exists(genre_dir) or len([f for f in os.listdir(genre_dir) if f.endswith('.png')]) < 200:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "if should_generate_spectrograms():\n",
    "    print(\"Generating spectrograms...\")\n",
    "    generate_spectrograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34a9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=SPEC_ROOT, transform=transform)\n",
    "targets = [full_dataset[i][1] for i in range(len(full_dataset))]\n",
    "indices = list(range(len(full_dataset)))\n",
    "\n",
    "train_val_idx, test_idx = train_test_split(indices, test_size=0.2, stratify=targets, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_val_idx, test_size=0.1, stratify=[targets[i] for i in train_val_idx], random_state=42)\n",
    "\n",
    "train_subset = Subset(full_dataset, train_idx)\n",
    "val_subset = Subset(full_dataset, val_idx)\n",
    "test_ds = Subset(full_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c75699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trying learning rate: 0.001 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohan/NNDL/music-genre-classifier/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rohan/NNDL/music-genre-classifier/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 1.6572\n",
      "Epoch 2/15, Loss: 1.1502\n",
      "Epoch 3/15, Loss: 0.8831\n",
      "Epoch 4/15, Loss: 0.7431\n",
      "Epoch 5/15, Loss: 0.5209\n",
      "Epoch 6/15, Loss: 0.3894\n",
      "Epoch 7/15, Loss: 0.2646\n",
      "Epoch 8/15, Loss: 0.2194\n",
      "Epoch 9/15, Loss: 0.1361\n",
      "Epoch 10/15, Loss: 0.1173\n",
      "Epoch 11/15, Loss: 0.1076\n",
      "Epoch 12/15, Loss: 0.0836\n",
      "Epoch 13/15, Loss: 0.1247\n",
      "Epoch 14/15, Loss: 0.0815\n",
      "Epoch 15/15, Loss: 0.0644\n",
      "Validation Accuracy: 76.88%\n",
      "\n",
      "=== Trying learning rate: 0.0001 ===\n",
      "Epoch 1/15, Loss: 1.7343\n",
      "Epoch 2/15, Loss: 0.8357\n",
      "Epoch 3/15, Loss: 0.4169\n",
      "Epoch 4/15, Loss: 0.1710\n",
      "Epoch 5/15, Loss: 0.0790\n",
      "Epoch 6/15, Loss: 0.0408\n",
      "Epoch 7/15, Loss: 0.0310\n",
      "Epoch 8/15, Loss: 0.0259\n",
      "Epoch 9/15, Loss: 0.0232\n",
      "Epoch 10/15, Loss: 0.0161\n",
      "Epoch 11/15, Loss: 0.0129\n",
      "Epoch 12/15, Loss: 0.0088\n",
      "Epoch 13/15, Loss: 0.0083\n",
      "Epoch 14/15, Loss: 0.0076\n",
      "Epoch 15/15, Loss: 0.0078\n",
      "Validation Accuracy: 76.25%\n",
      "\n",
      "Best Config: {'learning_rate': 0.001}, Best Validation Accuracy: 76.88%\n"
     ]
    }
   ],
   "source": [
    "for lr in LEARNING_RATES:\n",
    "    print(f\"\\n=== Trying learning rate: {lr} ===\")\n",
    "\n",
    "    model = SpectrogramResNet(num_classes=len(GENRES)).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss / len(train_subset):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.2%}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_config = {'learning_rate': lr}\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "print(f\"\\nBest Config: {best_config}, Best Validation Accuracy: {best_val_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c14b678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohan/NNDL/music-genre-classifier/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rohan/NNDL/music-genre-classifier/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 72.25%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(\"\\n--- Final Test Evaluation ---\")\n",
    "best_model = SpectrogramResNet(num_classes=len(GENRES)).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "best_model.eval()\n",
    "\n",
    "correct, total = 0, 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = best_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.2%}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=GENRES)\n",
    "disp.plot(xticks_rotation=45, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_resnet.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015f4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
